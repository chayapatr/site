---
title: One Paper Per Day 2024 (1/366)
pubDate: 1 Jan 2024
---

## A Single Vector Is Not Enough: Taxonomy Expansion via Box Embeddings

ACM Web Conference 2023 (WWW'23)\
[Link [Open Access]](https://dl.acm.org/doi/10.1145/3543507.3583310)

As in the name, this paper discusses taxonomy, a.k.a. the topic I'm very interested in right now (haha) regarding information science and classification.

Taxonomy (or, in another term, hierarchical classification) is used in many matters—from traditional science domains (as in biology) to various things in our daily lives, like shopping; for example, we may look at the categories of products organized on web stores as taxonomy, such as: `electronic equipment -> household items -> air conditioners`

But the problem arises in this era where information is exponentially growing; it's nearly impossible to do taxonomy by hand (imagine people arranging consistent product categories on Amazon, one item at a time). Creating an autonomous system is becoming more and more necessary.

One typically used method is embedding, i.e., turning words into vectors in vector space (or, in common parlance, a point in space. You might imagine that in a graph). Similar things will be close to each other, like the words "king" and "palace," "space" and "rocket," and such (note: what's fun is that when vectors are just lists of numbers, We can also add and subtract the embedding values of the words -> as in the classic example `"king" - "man" + woman ~= "queen"`).

However, the paper discusses that since a vector is just a point, doing a taxonomy by measuring the distance between each point (each keyword -- bottom left picture) doesn't make that much sense because the vector point has a "symmetric relation." In contrast, the main feature of taxonomy is an "asymmetric relation" -- because one point should be the parent of another, e.g., `a bird (parent) -> pigeon (child)`, which it cannot represent using only the likeness function between the two.

What the paper suggests is to do embedding and, instead of using the values directly, convert them into "boxes" (box) -- or in a more jargon-y term, "Hyperrectangle" -- first (like in the picture below right). Therefore, we can find each keyword's parent/child nature using the size property of boxes. For example, if Box A is in Box B, we can conclude that Box B is the parent of Box A.

As for details on how to convert embedding into a box or how to create a whole system, you can read it yourself. But it's surely a fun read.

[Original Post (Thai version) ↗](https://www.facebook.com/photo.php?fbid=2260353797502268)
